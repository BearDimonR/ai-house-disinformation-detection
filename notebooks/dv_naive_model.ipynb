{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "from typing import Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-18T21:56:26.795583Z",
     "end_time": "2023-11-18T21:56:27.873972Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-11-18T21:56:55.308869Z",
     "end_time": "2023-11-18T21:56:55.312235Z"
    }
   },
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "ROOT_DIR = '../datasets/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def seed_all(seed_value):\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "seed_all(RANDOM_SEED)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-18T21:56:55.514636Z",
     "end_time": "2023-11-18T21:56:55.517516Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(os.path.join(ROOT_DIR, 'train.csv'))\n",
    "test_df = pd.read_csv(os.path.join(ROOT_DIR, 'test.csv'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-18T22:27:47.036440Z",
     "end_time": "2023-11-18T22:27:47.068848Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Naive Bayes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/danylovanin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/danylovanin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "train_data, dev_data = train_test_split(train_df, test_size=0.2, random_state=RANDOM_SEED)\n",
    "test_data = test_df\n",
    "\n",
    "# Download the Snowball stemmer for Russian language\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Create a Snowball stemmer for Russian\n",
    "stemmer = SnowballStemmer(\"russian\")\n",
    "\n",
    "# Tokenize the text using NLTK for Russian language\n",
    "train_data['tokenized_content'] = train_data['Content'].apply(lambda x: [stemmer.stem(word) for word in word_tokenize(x, language='russian')])\n",
    "dev_data['tokenized_content'] = dev_data['Content'].apply(lambda x: [stemmer.stem(word) for word in word_tokenize(x, language='russian')])\n",
    "test_data['tokenized_content'] = test_data['Content'].apply(lambda x: [stemmer.stem(word) for word in word_tokenize(x, language='russian')])\n",
    "\n",
    "y_train = torch.tensor(train_data['Suspicious_Level'].values)\n",
    "y_dev = torch.tensor(dev_data['Suspicious_Level'].values)\n",
    "\n",
    "# Adjust labels to be in the range [0, num_classes - 1]\n",
    "y_train = y_train - 1\n",
    "y_dev = y_dev - 1\n",
    "\n",
    "# Vectorize the tokenized text using CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X_train = vectorizer.fit_transform(train_data['tokenized_content'].apply(lambda x: ' '.join(x)))\n",
    "X_dev = vectorizer.transform(dev_data['tokenized_content'].apply(lambda x: ' '.join(x)))\n",
    "X_test = vectorizer.transform(test_data['tokenized_content'].apply(lambda x: ' '.join(x)))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-18T22:27:47.905179Z",
     "end_time": "2023-11-18T22:27:52.813433Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0: 88 occurrences\n",
      "Class 1: 22 occurrences\n",
      "Class 2: 8 occurrences\n",
      "[1 1 0 2 2 1 0 1 0 0 0 1 0 0 2 1 0 1 0 0 0 2 0 1 2 0 1 0 0 0 0 2 0 1 0 0 1\n",
      " 0 0 0 0 1 0 0 1 1 0 2 2 0 2 0 0 0 0 2 0 0 2 0 0 0 0 0 0 1 2 1 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 2 1 0 1 0 0 0 2 0 0 2 1 0 1 0 0 1 1 0 0 0 1 0 0 0 1 0 1 0 2 0\n",
      " 0 0 0 0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "# Create a naive Bayes classifier\n",
    "class NaiveBayes(nn.Module):\n",
    "    def __init__(self, num_classes, num_features):\n",
    "        super(NaiveBayes, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.num_features = num_features\n",
    "        self.theta = nn.Parameter(torch.zeros(num_classes, num_features))\n",
    "        self.bias = nn.Parameter(torch.zeros(num_classes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        scores = x @ self.theta.t() + self.bias\n",
    "        return scores\n",
    "\n",
    "# Instantiate the model and set hyperparameters\n",
    "num_classes = 3  # Three classes: Positive, Negative, Neutral\n",
    "num_features = X_train.shape[1]\n",
    "model = NaiveBayes(num_classes, num_features)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train.toarray(), dtype=torch.float32)\n",
    "X_dev_tensor = torch.tensor(X_dev.toarray(), dtype=torch.float32)\n",
    "\n",
    "# Create DataLoader for training\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 500\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_dev_tensor)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "\n",
    "# Display the counts for each class\n",
    "class_counts = np.bincount(predicted)\n",
    "for class_label, count in enumerate(class_counts):\n",
    "    print(f\"Class {class_label}: {count} occurrences\")\n",
    "\n",
    "print(y_dev.numpy())\n",
    "# accuracy = accuracy_score(y_ev.numpy(), predicted.numpy())\n",
    "# print(f'Test Accuracy: {accuracy * 100:.2f}%')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-18T22:29:14.222599Z",
     "end_time": "2023-11-18T22:29:19.392863Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 77.97%\n"
     ]
    }
   ],
   "source": [
    "# print(len(y_dev.numpy()))\n",
    "# print(len(predicted.numpy()))\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_dev.numpy(), predicted.numpy())\n",
    "print(f'Test Accuracy: {accuracy * 100:.2f}%')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-18T22:29:23.298598Z",
     "end_time": "2023-11-18T22:29:23.301712Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'numpy.float64' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[77], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m f2_score \u001B[38;5;241m=\u001B[39m \u001B[43mf1_score\u001B[49m\u001B[43m(\u001B[49m\u001B[43my_dev\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnumpy\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpredicted\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnumpy\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maverage\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmacro\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTest F1 Score: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mf2_score\u001B[38;5;250m \u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m100\u001B[39m\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.2f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[0;31mTypeError\u001B[0m: 'numpy.float64' object is not callable"
     ]
    }
   ],
   "source": [
    "f2_score = f1_score(y_dev.numpy(), predicted.numpy(), average='macro')\n",
    "print(f'Test F1 Score: {f2_score * 100:.2f}%')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-18T22:24:46.300037Z",
     "end_time": "2023-11-18T22:24:46.303769Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [],
   "source": [
    "X_test_tensor = torch.tensor(X_test.toarray(), dtype=torch.float32)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test_tensor)\n",
    "    # print(outputs)\n",
    "    _, predicted = torch.max(outputs, 1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-18T22:29:28.536190Z",
     "end_time": "2023-11-18T22:29:28.558717Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [],
   "source": [
    "back_to_normal = predicted + 1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-18T22:29:29.974767Z",
     "end_time": "2023-11-18T22:29:29.980527Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [],
   "source": [
    "test_df['Suspicious_Level'] = back_to_normal"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-18T22:29:32.243740Z",
     "end_time": "2023-11-18T22:29:32.249017Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "data": {
      "text/plain": "Suspicious_Level\n1    983\n2    142\n3     46\nName: count, dtype: int64"
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['Suspicious_Level'].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-18T22:29:32.931039Z",
     "end_time": "2023-11-18T22:29:32.937175Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [],
   "source": [
    "test_df[['MessageId', 'Suspicious_Level']].to_csv('naive_bayes_updated.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-18T22:29:40.291616Z",
     "end_time": "2023-11-18T22:29:40.296115Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
