{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "from typing import Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-18T23:03:18.715553Z",
     "end_time": "2023-11-18T23:03:18.722101Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-11-18T23:16:48.104533Z",
     "end_time": "2023-11-18T23:16:48.112088Z"
    }
   },
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "ROOT_DIR = '../datasets/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [],
   "source": [
    "def seed_all(seed_value):\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "seed_all(RANDOM_SEED)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-18T23:16:48.351444Z",
     "end_time": "2023-11-18T23:16:48.356676Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(os.path.join(ROOT_DIR, 'train.csv'))\n",
    "test_df = pd.read_csv(os.path.join(ROOT_DIR, 'test.csv'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-18T23:16:48.857941Z",
     "end_time": "2023-11-18T23:16:48.889712Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Naive Bayes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/danylovanin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/danylovanin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk import tokenize\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "train_data, dev_data = train_test_split(train_df, test_size=0.2, random_state=RANDOM_SEED)\n",
    "test_data = test_df\n",
    "\n",
    "# Download the Snowball stemmer for Russian language\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Create a Snowball stemmer for Russian\n",
    "stemmer = SnowballStemmer(\"russian\")\n",
    "\n",
    "def collapse_dots(input):\n",
    "    # Collapse sequential dots\n",
    "    input = re.sub(\"\\.+\", \".\", input)\n",
    "    # Collapse dots separated by whitespaces\n",
    "    all_collapsed = False\n",
    "    while not all_collapsed:\n",
    "        output = re.sub(r\"\\.(( )*)\\.\", \".\", input)\n",
    "        all_collapsed = input == output\n",
    "        input = output\n",
    "    return output\n",
    "\n",
    "def process_text(input):\n",
    "    if isinstance(input, str):\n",
    "        input = \" \".join(tokenize.sent_tokenize(input))\n",
    "        input = re.sub(r\"http\\S+\", \"\", input)\n",
    "        input = re.sub(r\"\\n+\", \". \", input)\n",
    "        for symb in [\"!\", \",\", \":\", \";\", \"?\"]:\n",
    "            input = re.sub(rf\"\\{symb}\\.\", symb, input)\n",
    "        input = re.sub(\"[^а-яА-Яa-zA-Z0-9!\\\"#$%&'()*+,-./:;<=>?@[\\\\]^_`{|}~ё]+\", \" \", input)\n",
    "        input = re.sub(r\"#\\S+\", \"\", input)\n",
    "        input = collapse_dots(input)\n",
    "        input = input.strip()\n",
    "        # input = input.lower()\n",
    "    return input\n",
    "\n",
    "train_data[\"Content_processed\"] = train_data[\"Content\"].apply(process_text)\n",
    "dev_data[\"Content_processed\"] = dev_data[\"Content\"].apply(process_text)\n",
    "test_data[\"Content_processed\"] = test_data[\"Content\"].apply(process_text)\n",
    "\n",
    "# Tokenize the text using NLTK for Russian language\n",
    "train_data['tokenized_content'] = train_data['Content_processed'].apply(lambda x: [stemmer.stem(word) for word in word_tokenize(x, language='russian')])\n",
    "dev_data['tokenized_content'] = dev_data['Content_processed'].apply(lambda x: [stemmer.stem(word) for word in word_tokenize(x, language='russian')])\n",
    "test_data['tokenized_content'] = test_data['Content_processed'].apply(lambda x: [stemmer.stem(word) for word in word_tokenize(x, language='russian')])\n",
    "\n",
    "y_train = torch.tensor(train_data['Suspicious_Level'].values)\n",
    "y_dev = torch.tensor(dev_data['Suspicious_Level'].values)\n",
    "\n",
    "# Adjust labels to be in the range [0, num_classes - 1]\n",
    "y_train = y_train - 1\n",
    "y_dev = y_dev - 1\n",
    "\n",
    "# Vectorize the tokenized text using CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X_train = vectorizer.fit_transform(train_data['tokenized_content'].apply(lambda x: ' '.join(x)))\n",
    "X_dev = vectorizer.transform(dev_data['tokenized_content'].apply(lambda x: ' '.join(x)))\n",
    "X_test = vectorizer.transform(test_data['tokenized_content'].apply(lambda x: ' '.join(x)))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-18T23:16:50.323883Z",
     "end_time": "2023-11-18T23:16:55.622146Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0: 86 occurrences\n",
      "Class 1: 23 occurrences\n",
      "Class 2: 9 occurrences\n",
      "[1 1 0 2 2 1 0 1 0 0 0 1 0 0 2 1 0 1 0 0 0 2 0 1 2 0 1 0 0 0 0 2 0 1 0 0 1\n",
      " 0 0 0 0 1 0 0 1 1 0 2 2 0 2 0 0 0 0 2 0 0 2 0 0 0 0 0 0 1 2 1 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 2 1 0 1 0 0 0 2 0 0 2 1 0 1 0 0 1 1 0 0 0 1 0 0 0 1 0 1 0 2 0\n",
      " 0 0 0 0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "# Create a naive Bayes classifier\n",
    "class NaiveBayes(nn.Module):\n",
    "    def __init__(self, num_classes, num_features, alpha=1.0):\n",
    "        super(NaiveBayes, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.num_features = num_features\n",
    "        self.alpha = alpha  # Laplace smoothing parameter\n",
    "\n",
    "        # Parameters for the likelihoods\n",
    "        self.theta = nn.Parameter(torch.zeros(num_classes, num_features))\n",
    "        self.bias = nn.Parameter(torch.zeros(num_classes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        scores = x @ self.theta.t() + self.bias\n",
    "        return scores\n",
    "\n",
    "    def laplace_smoothing(self, x):\n",
    "        # Apply Laplace smoothing to the likelihoods\n",
    "        return (x + self.alpha) / (x.sum(dim=1, keepdim=True) + self.alpha * self.num_features)\n",
    "\n",
    "\n",
    "# Instantiate the model and set hyperparameters\n",
    "num_classes = 3  # Three classes: Positive, Negative, Neutral\n",
    "num_features = X_train.shape[1]\n",
    "laplace_alpha = 1.0  # You can adjust this parameter\n",
    "model = NaiveBayes(num_classes, num_features, alpha=laplace_alpha)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train.toarray(), dtype=torch.float32)\n",
    "X_dev_tensor = torch.tensor(X_dev.toarray(), dtype=torch.float32)\n",
    "\n",
    "# Create DataLoader for training\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 700\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        likelihoods = model.laplace_smoothing(torch.exp(outputs))\n",
    "        log_likelihoods = torch.log(likelihoods)\n",
    "        loss = criterion(log_likelihoods, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_dev_tensor)\n",
    "    likelihoods = model.laplace_smoothing(torch.exp(outputs))\n",
    "    _, predicted = torch.max(likelihoods, 1)\n",
    "\n",
    "\n",
    "# Display the counts for each class\n",
    "class_counts = np.bincount(predicted)\n",
    "for class_label, count in enumerate(class_counts):\n",
    "    print(f\"Class {class_label}: {count} occurrences\")\n",
    "\n",
    "print(y_dev.numpy())\n",
    "# accuracy = accuracy_score(y_ev.numpy(), predicted.numpy())\n",
    "# print(f'Test Accuracy: {accuracy * 100:.2f}%')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-18T23:16:55.627576Z",
     "end_time": "2023-11-18T23:17:03.955870Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118\n",
      "118\n",
      "Test Accuracy: 77.97%\n"
     ]
    }
   ],
   "source": [
    "print(len(y_dev.numpy()))\n",
    "print(len(predicted.numpy()))\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_dev.numpy(), predicted.numpy())\n",
    "print(f'Test Accuracy: {accuracy * 100:.2f}%')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-18T23:17:03.956090Z",
     "end_time": "2023-11-18T23:17:03.958178Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test F1 Score: 63.15%\n"
     ]
    }
   ],
   "source": [
    "f2_score = f1_score(y_dev.numpy(), predicted.numpy(), average='macro')\n",
    "print(f'Test F1 Score: {f2_score * 100:.2f}%')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-18T23:17:07.788796Z",
     "end_time": "2023-11-18T23:17:07.798876Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [],
   "source": [
    "X_test_tensor = torch.tensor(X_test.toarray(), dtype=torch.float32)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test_tensor)\n",
    "    likelihoods = model.laplace_smoothing(torch.exp(outputs))\n",
    "    _, predicted = torch.max(likelihoods, 1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-18T23:17:09.598339Z",
     "end_time": "2023-11-18T23:17:09.626832Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [],
   "source": [
    "back_to_normal = predicted + 1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-18T23:17:10.897900Z",
     "end_time": "2023-11-18T23:17:10.907139Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "outputs": [],
   "source": [
    "test_df['Suspicious_Level'] = back_to_normal"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-18T23:17:11.560265Z",
     "end_time": "2023-11-18T23:17:11.568557Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [
    {
     "data": {
      "text/plain": "Suspicious_Level\n1    980\n2    139\n3     52\nName: count, dtype: int64"
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['Suspicious_Level'].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-18T23:17:12.040562Z",
     "end_time": "2023-11-18T23:17:12.061417Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "outputs": [],
   "source": [
    "test_df[['MessageId', 'Suspicious_Level']].to_csv('naive_bayes_laplace_normalised.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-18T23:17:21.427148Z",
     "end_time": "2023-11-18T23:17:21.431182Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
